{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Author : Xingmin Liu\n",
    "# @Time : 2023.11.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a878a28-eade-4765-9c56-d744c47a49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "class RealORFakeDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        # 对非数值型特征进行编码\n",
    "        for column in ['is_noisy', 'has_repeating_patterns', 'is_symmetric']:\n",
    "            self.data[column] = self.data[column].replace({'True': 1, 'False': 0})\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 加载并预处理图像\n",
    "        image = Image.open(self.data.iloc[index]['image_path']).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # 获取其他特征\n",
    "        other_features = self.data.iloc[index].drop(['image_path', 'label']).values.astype(float)\n",
    "        other_features = torch.tensor(other_features, dtype=torch.float)\n",
    "\n",
    "        # 获取标签\n",
    "        label = self.data.iloc[index]['label']\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, other_features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d3ca543-d137-43bc-b9e2-1c264241bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# 定义图像预处理\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "    transforms.RandomVerticalFlip(),  # 随机垂直翻转\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(40),  # 在[-30, 30]范围内随机旋转\n",
    "    transforms.RandomAffine(degrees=0, shear=10, scale=(0.8,1.2)),  # 随机仿射变换\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # 色彩抖动\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(),  # 随机擦除\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 创建数据集实例\n",
    "train_dataset = RealORFakeDataset('./_train_final_s.csv', transform=train_transform)\n",
    "valid_dataset = RealORFakeDataset('./_valid_final_s.csv', transform=valid_transform)\n",
    "\n",
    "# 创建 DataLoader 实例\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09e69b5c-a60b-4388-a7fe-541d6854a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_other_features):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # 图像输入分支 (ResNet50)\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()  # Remove the final fc layer.\n",
    "\n",
    "        # 非图像输入分支\n",
    "        self.other_features_layer = nn.Sequential(\n",
    "            nn.Linear(num_other_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        # 输出层\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(num_features + 128, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, image_inputs, other_inputs):\n",
    "        image_features = self.resnet(image_inputs)\n",
    "        other_features = self.other_features_layer(other_inputs)\n",
    "        concatenated = torch.cat([image_features, other_features], dim=1)\n",
    "        return self.output_layer(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6285a0-f21e-4b29-9a9c-3303cbbd3a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/environment/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# 假设 MyModel 已经被定义\n",
    "num_other_features = 12\n",
    "model = MyModel(num_other_features=num_other_features)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60c91222-40b0-4345-b7a9-1af438ab0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def train(model, train_loader, val_loader, optimizer, num_epochs):\n",
    "    # 切换模型到训练模式\n",
    "    model.train()\n",
    "\n",
    "    # 用于保存每个批次的 loss 和 F1 分数\n",
    "    train_losses = []\n",
    "    train_f1s = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_losses = []\n",
    "        epoch_labels = []\n",
    "        epoch_outputs = []\n",
    "\n",
    "        # 添加 tqdm 进度条\n",
    "        for images, other_features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Training)\"):\n",
    "            # 将数据移动到设备上\n",
    "            images = images.to(device)\n",
    "            other_features = other_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 清空优化器的梯度\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(images, other_features)\n",
    "            loss = torch.nn.BCELoss()(outputs.squeeze(), labels.float())\n",
    "\n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 保存每个批次的 loss\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "            # 保存输出和标签以计算 F1 分数\n",
    "            epoch_labels.extend(labels.detach().cpu().numpy())\n",
    "            epoch_outputs.extend((outputs.detach().cpu().numpy() > 0.5).astype(int))  # 设置阈值为 0.5\n",
    "\n",
    "        # 保存本 epoch 的所有批次的 loss\n",
    "        train_losses.append(batch_losses)\n",
    "\n",
    "        # 计算本 epoch 的 F1 分数，然后保存\n",
    "        epoch_f1 = f1_score(epoch_labels, epoch_outputs, average='macro')\n",
    "        train_f1s.append(epoch_f1)\n",
    "\n",
    "        # 评估验证集\n",
    "        model.eval()  # 切换模型为评估模式\n",
    "        with torch.no_grad():\n",
    "            val_epoch_losses = []\n",
    "            val_epoch_labels = []\n",
    "            val_epoch_outputs = []\n",
    "\n",
    "            for val_images, val_other_features, val_labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Validation)\"):\n",
    "                # 将数据移动到设备上\n",
    "                val_images = val_images.to(device)\n",
    "                val_other_features = val_other_features.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "\n",
    "                # 前向传播\n",
    "                val_outputs = model(val_images, val_other_features)\n",
    "                val_loss = torch.nn.BCELoss()(val_outputs.squeeze(), val_labels.float())\n",
    "\n",
    "                # 保存 loss\n",
    "                val_epoch_losses.append(val_loss.item())\n",
    "\n",
    "                # 保存输出和标签以计算 F1 分数\n",
    "                val_epoch_labels.extend(val_labels.detach().cpu().numpy())\n",
    "                val_epoch_outputs.extend((val_outputs.detach().cpu().numpy() > 0.5).astype(int))  # 设置阈值为 0.5\n",
    "\n",
    "            # 计算本 epoch 的平均 loss 和 F1 分数，然后保存\n",
    "            val_epoch_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
    "            val_losses.append(val_epoch_loss)\n",
    "            val_epoch_f1 = f1_score(val_epoch_labels, val_epoch_outputs, average='macro')\n",
    "            val_f1s.append(val_epoch_f1)\n",
    "\n",
    "        model.train()  # 切换模型回训练模式\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {sum(batch_losses)/len(batch_losses)}, Train F1 Score: {epoch_f1}, Val Loss: {val_epoch_loss}, Val F1 Score: {val_epoch_f1}')\n",
    "\n",
    "    return train_losses, train_f1s, val_losses, val_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60dc5c48-5d16-459c-a440-c313c0dd7885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 12])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "for val_images, val_other_features, val_labels in valid_loader:\n",
    "    print(val_other_features.shape)\n",
    "    # 将数据移动到设备上\n",
    "    val_images = val_images.to(device)\n",
    "    val_other_features = val_other_features.to(device)\n",
    "    val_labels = val_labels.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "410c46d2-4727-4f1e-9a1c-09c3237a511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Training): 100%|██████████| 50/50 [01:55<00:00,  2.31s/it]\n",
      "Epoch 1/10 (Validation): 100%|██████████| 13/13 [00:12<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.1216692403703928, Train F1 Score: 0.9509373550639639, Val Loss: 0.0588879083068325, Val F1 Score: 0.9809340573312505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Training): 100%|██████████| 50/50 [01:52<00:00,  2.25s/it]\n",
      "Epoch 2/10 (Validation): 100%|██████████| 13/13 [00:12<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.055252220816910266, Train F1 Score: 0.9815624635390514, Val Loss: 0.030736291136306066, Val F1 Score: 0.9890617212885566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Training): 100%|██████████| 50/50 [01:49<00:00,  2.19s/it]\n",
      "Epoch 3/10 (Validation): 100%|██████████| 13/13 [00:11<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.04808330893516541, Train F1 Score: 0.9814843658461526, Val Loss: 0.04015234615116452, Val F1 Score: 0.9856220289270712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 (Training): 100%|██████████| 50/50 [01:51<00:00,  2.24s/it]\n",
      "Epoch 4/10 (Validation): 100%|██████████| 13/13 [00:12<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.041393141131848096, Train F1 Score: 0.9849218741717338, Val Loss: 0.025760719301895454, Val F1 Score: 0.9918744635876353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 (Training): 100%|██████████| 50/50 [01:47<00:00,  2.15s/it]\n",
      "Epoch 5/10 (Validation): 100%|██████████| 13/13 [00:11<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.04026844147592783, Train F1 Score: 0.9864062486724852, Val Loss: 0.02559427113737911, Val F1 Score: 0.990312226585301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 (Training): 100%|██████████| 50/50 [01:49<00:00,  2.20s/it]\n",
      "Epoch 6/10 (Validation): 100%|██████████| 13/13 [00:12<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.03469134472310543, Train F1 Score: 0.9872656211915005, Val Loss: 0.020005001077571742, Val F1 Score: 0.9924996454910564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 (Training): 100%|██████████| 50/50 [01:47<00:00,  2.15s/it]\n",
      "Epoch 7/10 (Validation): 100%|██████████| 13/13 [00:11<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.03324458198621869, Train F1 Score: 0.986640606653665, Val Loss: 0.02184639308744898, Val F1 Score: 0.9921874076832353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 (Training): 100%|██████████| 50/50 [01:47<00:00,  2.15s/it]\n",
      "Epoch 8/10 (Validation): 100%|██████████| 13/13 [00:11<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.03193630635738373, Train F1 Score: 0.988828105293716, Val Loss: 0.02348239958071365, Val F1 Score: 0.9909372442249591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 (Training): 100%|██████████| 50/50 [01:53<00:00,  2.26s/it]\n",
      "Epoch 9/10 (Validation): 100%|██████████| 13/13 [00:12<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.028129221154376865, Train F1 Score: 0.990078117672438, Val Loss: 0.016159846178757455, Val F1 Score: 0.9943749978027335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 (Training): 100%|██████████| 50/50 [01:51<00:00,  2.22s/it]\n",
      "Epoch 10/10 (Validation): 100%|██████████| 13/13 [00:12<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.026241997880861165, Train F1 Score: 0.9908593736052511, Val Loss: 0.021637405738986742, Val F1 Score: 0.991561984984435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 检查是否有可用的 GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 将模型移到设备上\n",
    "model = model.to(device)\n",
    "\n",
    "# 开始训练\n",
    "# train_losses, train_f1s, val_losses, val_f1s = train(model, train_loader, valid_loader, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6fc41c-cab9-460d-938a-f0546594cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设 train_losses, train_f1s, val_losses, val_f1s 是四个列表，包含了训练和验证过程的损失和 F1 分数\n",
    "\n",
    "# 创建一个新的 figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 绘制训练损失\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 绘制训练 F1 分数\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train_f1s, label='Training F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Training F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "# 绘制验证损失\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 绘制验证 F1 分数\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(val_f1s, label='Validation F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Validation F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "# 显示 figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3905219a-0f5c-47a5-bd15-f27219229cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model_path = \"MutiResNet.pth\"\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b6353b-821f-478c-9495-b490b50c970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from utils import noise\n",
    "from utils import repeat\n",
    "from utils import symmetric_sift\n",
    "from utils import naturalF\n",
    "\n",
    "# is_noisy\n",
    "# noise_ratio\n",
    "# noise_level\n",
    "# has_repeating_patterns\n",
    "# is_symmetric\n",
    "# good_matches_count\n",
    "# brightness\n",
    "# contrast\n",
    "# saturation\n",
    "# h_entropy\n",
    "# s_entropy\n",
    "# v_entropy\n",
    "# label\n",
    "# TRUE,0.202346802,3,TRUE,FALSE,19,114.3035736,56.91729966,102.1498413,5.090866089,6.892580509,7.659484863,0\n",
    "# 定义一个函数来对单张图片进行预测\n",
    "from PIL import Image\n",
    "def predict_image(model, image_path):\n",
    "    model.eval()  # 切换模型为评估模式\n",
    "    # 加载图片\n",
    "    image = Image.open(image_path)\n",
    "    if image.format == 'PNG':\n",
    "        # 将 RGBA 转换为 RGB，忽略 alpha 通道\n",
    "        image = image.convert('RGB')\n",
    "    _, _, _, noise_ratio, is_noisy, noise_level = noise.noise_detection_17_40(image_path)\n",
    "    is_noisy = int(is_noisy)\n",
    "    noise_level = int(noise_level)\n",
    "    _, has_repeating_patterns, _ = repeat.detect_repeating_patterns_21_30(image_path)\n",
    "    has_repeating_patterns = int(has_repeating_patterns)\n",
    "    is_symmetric, good_matches_count = symmetric_sift.is_symmetric_sift_16_55(image_path)\n",
    "    is_symmetric = int(is_symmetric)\n",
    "    check = naturalF.is_natural_color_histogram_advanced_14_00(image_path)\n",
    "    brightness, contrast, saturation, h_entropy, s_entropy, v_entropy = check.values()\n",
    "    other_input = [[noise_ratio, is_noisy, noise_level, has_repeating_patterns, is_symmetric, good_matches_count, brightness, contrast, saturation, h_entropy, s_entropy, v_entropy]]\n",
    "    # print(other_input)\n",
    "    other_input_tensor = torch.Tensor(other_input)\n",
    "\n",
    "    # 定义转换\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 使用与训练模型时相同的归一化参数\n",
    "    ])\n",
    "\n",
    "    # 对图片进行转换\n",
    "    image = transform(image)\n",
    "\n",
    "    # 添加一个维度来表示批量大小（因为 PyTorch 总是期望有一个批量维度）\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # 将图片移动到设备上\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    image = image.to(device)\n",
    "    other_input_tensor = other_input_tensor.to(device)\n",
    "\n",
    "    # 使用模型进行预测\n",
    "    output = model(image, other_input_tensor)\n",
    "    output = output.item()\n",
    "    if output > 0.5:\n",
    "        output_class = 1\n",
    "    else:\n",
    "        output_class = 0\n",
    "\n",
    "    # 返回预测结果\n",
    "    return output, output_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09206a04-19be-4576-9357-f5a47b31c2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:   2%|▏         | 90/4000 [03:01<2:05:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough descriptors found in one or both of the images. Skipping ./input/newest_test/test_1079.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  13%|█▎        | 509/4000 [17:30<2:03:55,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one or both of the images. Skipping ./input/newest_test/test_1456.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  16%|█▌        | 635/4000 [21:46<1:44:10,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one or both of the images. Skipping ./input/newest_test/test_157.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  21%|██        | 821/4000 [27:37<1:45:54,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one or both of the images. Skipping ./input/newest_test/test_1737.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  25%|██▌       | 1007/4000 [33:40<1:33:05,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough descriptors found in one or both of the images. Skipping ./input/newest_test/test_1904.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  27%|██▋       | 1062/4000 [35:29<1:33:06,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one or both of the images. Skipping ./input/newest_test/test_1954.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  28%|██▊       | 1123/4000 [37:25<1:28:35,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough descriptors found in one or both of the images. Skipping ./input/newest_test/test_2008.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  29%|██▉       | 1156/4000 [38:27<1:30:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough descriptors found in one or both of the images. Skipping ./input/newest_test/test_2038.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  33%|███▎      | 1323/4000 [43:37<1:20:19,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough descriptors found in one or both of the images. Skipping ./input/newest_test/test_2189.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  35%|███▌      | 1403/4000 [46:08<1:22:22,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one or both of the images. Skipping ./input/newest_test/test_2260.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  38%|███▊      | 1529/4000 [50:16<1:19:10,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough descriptors found in one or both of the images. Skipping ./input/newest_test/test_2374.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  39%|███▉      | 1554/4000 [51:06<1:24:49,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough descriptors found in one or both of the images. Skipping ./input/newest_test/test_2397.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  51%|█████     | 2040/4000 [1:06:42<58:56,  1.80s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one or both of the images. Skipping ./input/newest_test/test_2834.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  58%|█████▊    | 2310/4000 [1:15:17<53:16,  1.89s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one or both of the images. Skipping ./input/newest_test/test_3077.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  61%|██████▏   | 2454/4000 [1:19:50<49:08,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough descriptors found in one or both of the images. Skipping ./input/newest_test/test_3206.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  68%|██████▊   | 2725/4000 [1:28:20<37:54,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one or both of the images. Skipping ./input/newest_test/test_3450.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  69%|██████▉   | 2753/4000 [1:29:11<38:04,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one or both of the images. Skipping ./input/newest_test/test_3476.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  70%|██████▉   | 2786/4000 [1:30:12<38:20,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough descriptors found in one or both of the images. Skipping ./input/newest_test/test_3505.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  94%|█████████▍| 3780/4000 [2:01:57<06:43,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one or both of the images. Skipping ./input/newest_test/test_80.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  95%|█████████▍| 3782/4000 [2:02:01<06:40,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one or both of the images. Skipping ./input/newest_test/test_801.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images:  99%|█████████▉| 3977/4000 [2:08:21<00:42,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough descriptors found in one or both of the images. Skipping ./input/newest_test/test_978.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting images: 100%|██████████| 4000/4000 [2:09:06<00:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction time: 7746.987056493759 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def predict_images_in_folder(model, folder_path, output_csv_path):\n",
    "    # 获取文件夹中的所有文件\n",
    "    image_files = os.listdir(folder_path)\n",
    "\n",
    "    # 创建 CSV 文件，准备写入预测结果\n",
    "    with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['image_path', 'prediction', 'prediction_class']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "        # 记录开始时间\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 对每个文件进行预测\n",
    "        for image_file in tqdm(image_files, desc=\"Predicting images\"):\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            prediction, prediction_class = predict_image(model, image_path)\n",
    "\n",
    "            # 写入预测结果\n",
    "            writer.writerow({'image_path': image_path, 'prediction': prediction, 'prediction_class': prediction_class})\n",
    "\n",
    "        # 计算并输出预测时间\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Total prediction time: {elapsed_time} seconds')\n",
    "\n",
    "# 使用方法\n",
    "model = torch.load('./MutiResNet_1116.pth')\n",
    "folder_path = './input/newest_test/'  # 图片文件夹的路径\n",
    "output_csv_path = './output/predictions.csv'  # 输出 CSV 的路径\n",
    "predict_images_in_folder(model, folder_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f29945e-a867-47ad-8c1e-43731fc23e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9982004165649414"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(model, image_path='./data/dataset/test/real/test_real_4.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
